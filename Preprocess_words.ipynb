{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2773f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentimentNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for vaderSentiment from https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\ivanb\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ivanb\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivanb\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ivanb\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivanb\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.12.14)\n",
      "Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "# pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077514f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b984b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiments(filepath):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath, sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "    # Extract year from 'time' column\n",
    "    df['year'] = pd.to_datetime(df['time'], errors='coerce').dt.year\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Columns with text to analyze\n",
    "    text_columns = ['person', 'location', 'organization', 'miscellaneous']\n",
    "    categorized_words = {col: {} for col in text_columns}  # Store words per category\n",
    "\n",
    "    # Function to clean words (remove symbols, keep letters and spaces)\n",
    "    def clean_word(word):\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', word)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        year = row['year']\n",
    "        \n",
    "        if pd.notna(year):  # Ensure valid year\n",
    "            for col in text_columns:\n",
    "                if pd.notna(row[col]):\n",
    "                    words = str(row[col]).split('|')\n",
    "                    for word in words:\n",
    "                        word = word.strip()\n",
    "                        cleaned_word = clean_word(word)  # Clean word\n",
    "                        if cleaned_word:  # Process cleaned word\n",
    "                            sentiment = analyzer.polarity_scores(cleaned_word)\n",
    "                            normalized_score = (sentiment['compound'] + 1) / 2\n",
    "                            \n",
    "                            if cleaned_word not in categorized_words[col]:\n",
    "                                categorized_words[col][cleaned_word] = {'count': 0, 'total_sentiment': 0, 'years': set()}\n",
    "                            categorized_words[col][cleaned_word]['count'] += 1\n",
    "                            categorized_words[col][cleaned_word]['total_sentiment'] += normalized_score\n",
    "                            categorized_words[col][cleaned_word]['years'].add(year)  # Track years\n",
    "\n",
    "    # Create results\n",
    "    results = []\n",
    "    for category, words in categorized_words.items():\n",
    "        for word, stats in words.items():\n",
    "            avg_sentiment = stats['total_sentiment'] / stats['count']\n",
    "            for year in stats['years']:\n",
    "                results.append({\n",
    "                    'year': year,\n",
    "                    'category': category,\n",
    "                    'text': word,\n",
    "                    'frequency': stats['count'],\n",
    "                    'sentiment': avg_sentiment\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame and sort by year, then category, then frequency\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=['year', 'category', 'frequency'], ascending=[True, True, False])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83eacf30",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'person'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'person'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m input_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/IMDB.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with desired dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Analyze sentiments\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m analyze_sentiments(input_filepath)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save the output\u001b[39;00m\n\u001b[0;32m      9\u001b[0m base_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(input_filepath))[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m, in \u001b[0;36manalyze_sentiments\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(year):  \u001b[38;5;66;03m# Ensure valid year\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(row[col]):\n\u001b[0;32m     25\u001b[0m             words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[col])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'person'"
     ]
    }
   ],
   "source": [
    "# Run analysis\n",
    "if __name__ == \"__main__\":\n",
    "    input_filepath = 'data/IMDB.tsv'  # Replace with desired dataset\n",
    "    \n",
    "    # Analyze sentiments\n",
    "    df = analyze_sentiments(input_filepath)\n",
    "    \n",
    "    # Save the output\n",
    "    base_name = os.path.splitext(os.path.basename(input_filepath))[0]\n",
    "    output_filepath = os.path.join(os.path.dirname(input_filepath), f\"{base_name}_sentiment.tsv\")\n",
    "    df.to_csv(output_filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42552aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
